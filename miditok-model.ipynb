{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install miditok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from miditok import REMI\n",
    "from miditok import MusicTokenizer, TokSequence\n",
    "from miditok.classes import TokenizerConfig\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x207c0ddf2f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the random seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\ianmh\\.cache\\kagglehub\\datasets\\soumikrakshit\\classical-music-midi\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# download the dataset and set path\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"soumikrakshit/classical-music-midi\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "artist = \"chopin\"\n",
    "midi_folder = os.path.join(path, artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to all midi files\n",
    "cwd = os.getcwd()\n",
    "midi_files = [\n",
    "    Path(midi_folder) / file\n",
    "    for file in os.listdir(midi_folder)\n",
    "    if file.endswith(\".mid\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer to use REMI+\n",
    "tokenizer: MusicTokenizer = REMI(\n",
    "    tokenizer_config=TokenizerConfig(\n",
    "        use_programs=True,\n",
    "        one_token_stream_for_programs=True,\n",
    "        use_time_signatures=True,\n",
    "    )\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer on the midi files\n",
    "tokenizer.train(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    model=\"BPE\",\n",
    "    files_paths=midi_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the midi files\n",
    "tokenized_midi_files = []\n",
    "for file in midi_files:\n",
    "    tok_seq: TokSequence = tokenizer.encode(file) # type: ignore\n",
    "    tokenized_midi_files.append(tok_seq.ids) # using ids to get the integer representation of the tokens can covert back with decode\n",
    "\n",
    "# Convert the list to a NumPy array if needed\n",
    "tokenized_midi_files = np.array(tokenized_midi_files, dtype=object)\n",
    "print(tokenized_midi_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 512\n",
    "stride = max_seq_len // 2\n",
    "\n",
    "def create_chunks(tokens: list[int], max_seq_len: int, stride: int) -> list[list[int]]:\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - max_seq_len, stride):\n",
    "        chunk = tokens[i:i + max_seq_len]\n",
    "        if len(chunk) != max_seq_len:\n",
    "            print(\"error\")\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# get a list of all the chunks regardless of the midi file\n",
    "tokenized_chunks = []\n",
    "for tok_seq in tokenized_midi_files:\n",
    "    chunks = create_chunks(tok_seq, max_seq_len, stride)\n",
    "\n",
    "    # add the chunks to the list of chunks\n",
    "    tokenized_chunks.extend(chunks)\n",
    "\n",
    "# convert the list to a NumPy array\n",
    "tokenized_chunks = np.array(tokenized_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input and target data from the chunks using a autoregressive model\n",
    "input_data = [chunk[:-1] for chunk in tokenized_chunks]\n",
    "target_data = [chunk[1:] for chunk in tokenized_chunks]\n",
    "\n",
    "# convert the input and target data to tensors\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.long)\n",
    "target_tensor = torch.tensor(target_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Spilt the data into training and validation and testing sets for inputs and targets\n",
    "train_input, vt_input, train_target, vt_target = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=seed)\n",
    "val_input, test_input, val_target, test_target = train_test_split(vt_input, vt_target, test_size=0.5, random_state=seed)\n",
    "\n",
    "# Create the dataset class\n",
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.inputs[idx], \"labels\": self.targets[idx]}\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = MidiDataset(train_input, train_target)\n",
    "val_dataset = MidiDataset(val_input, val_target)\n",
    "test_dataset = MidiDataset(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Define model configuration\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,  # Total number of unique tokens\n",
    "    n_positions=max_seq_len,  # Max sequence length\n",
    "    n_embd=768,  # Embedding size\n",
    "    n_layer=12,  # Number of transformer layers\n",
    "    n_head=12   # Number of attention heads\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Output directory to save model and logs\n",
    "    num_train_epochs=3,  # Number of epochs\n",
    "    per_device_train_batch_size=8,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # Batch size per device during evaluation\n",
    "    warmup_steps=500,    # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,   # Strength of weight decay\n",
    "    logging_dir='./logs', # Directory for storing logs\n",
    "    logging_steps=10,    # Log every 10 steps\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model when finished training\n",
    "    metric_for_best_model=\"loss\",  # Metric to track for best model\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # Your GPT2 model\n",
    "    args=training_args,  # The training arguments\n",
    "    train_dataset=train_dataset,  # The training set (TensorDataset)\n",
    "    eval_dataset=val_dataset,  # The validation set (TensorDataset)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the final model path\n",
    "final_model_path = f\"./{artist}_final_model\"\n",
    "\n",
    "# check if the final model is saved\n",
    "if not os.path.exists(final_model_path):\n",
    "    trainer.train()\n",
    "    trainer.save_model(final_model_path)  # Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer\n",
    "\n",
    "# load the final model from the saved model\n",
    "model = GPT2LMHeadModel.from_pretrained(final_model_path)\n",
    "\n",
    "# evaluate the model on the test dataset using the loaded model\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to be evaluated\n",
    "    args=training_args,  # The evaluation arguments\n",
    "    eval_dataset=test_dataset,  # The test dataset\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the final model from the saved model\n",
    "model = GPT2LMHeadModel.from_pretrained(final_model_path)\n",
    "\n",
    "# get the first 512 tokens from the test dataset\n",
    "seed_tokens = test_dataset[0][\"input_ids\"].unsqueeze(0)\n",
    "\n",
    "# create the input_ids tensor\n",
    "input_ids = seed_tokens\n",
    "\n",
    "# function to generate 1 new id at end of the input_ids tensor\n",
    "def generated_ids(ids):\n",
    "    # Generate tokens\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=ids,  # Your seed sequence\n",
    "        max_length=max_seq_len,  # Maximum length of generated song (in tokens)\n",
    "        num_return_sequences=1,  # Number of sequences to generate\n",
    "        no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        temperature=1.0,  # Sampling temperature for randomness (higher = more randomness)\n",
    "        top_p=0.95,  # Use top-p sampling for diversity\n",
    "        top_k=50,  # Use top-k sampling for diversity\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Padding token id if needed\n",
    "    )\n",
    "\n",
    "    return generated_ids[0].tolist()  # Get the generated tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symusic.core import ScoreTick\n",
    "\n",
    "# generated midi\n",
    "generated_midi = generated_ids(input_ids)\n",
    "\n",
    "# get the tokens from the generated tokens\n",
    "midi_sequence: ScoreTick = tokenizer.decode(generated_midi)\n",
    "print(midi_sequence)\n",
    "\n",
    "# get the path to the output file\n",
    "midi_file = \"multi_generated_midi.mid\"\n",
    "midi_path = os.path.join(cwd, midi_file)\n",
    "\n",
    "# save the midi sequence to a midi file\n",
    "midi_sequence.dump_midi(midi_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
