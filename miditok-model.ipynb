{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install miditok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from miditok import REMI\n",
    "from miditok import MusicTokenizer, TokSequence\n",
    "from miditok.classes import TokenizerConfig\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x207c0ddf2f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the random seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\ianmh\\.cache\\kagglehub\\datasets\\soumikrakshit\\classical-music-midi\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# download the dataset and set path\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"soumikrakshit/classical-music-midi\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "artist = \"chopin\"\n",
    "midi_folder = os.path.join(path, artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to all midi files\n",
    "cwd = os.getcwd()\n",
    "midi_files = [\n",
    "    Path(midi_folder) / file\n",
    "    for file in os.listdir(midi_folder)\n",
    "    if file.endswith(\".mid\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianmh\\Projects\\school\\cse144\\.venv\\Lib\\site-packages\\miditok\\tokenizations\\remi.py:77: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "# Define tokenizer to use REMI+\n",
    "tokenizer: MusicTokenizer = REMI(\n",
    "    tokenizer_config=TokenizerConfig(\n",
    "        use_programs=True,\n",
    "        one_token_stream_for_programs=True,\n",
    "        use_time_signatures=True,\n",
    "    )\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianmh\\AppData\\Local\\Temp\\ipykernel_20116\\185507970.py:2: UserWarning: miditok - tokenizer.train: `vocab_size` (484) need to be higher than the number of base tokens (484). Skipping tokenizer training.\n",
      "  tokenizer.train(\n"
     ]
    }
   ],
   "source": [
    "# Train the tokenizer on the midi files\n",
    "tokenizer.train(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    model=\"BPE\",\n",
    "    files_paths=midi_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 483, 189, 346, 20, 104, 125, 190, 346, 27, 100, 125, 346, 39, 101, 129, 192, 346, 36, 101, 125, 346, 44, 105, 125, 193, 346, 48, 101, 125, 346, 51, 106, 127, 194, 346, 44, 103, 125, 196, 346, 41, 99, 125, 346, 53, 103, 125, 197, 346, 19, 105, 125, 198, 346, 27, 102, 125, 346, 39, 104, 129, 200, 346, 37, 101, 125, 346, 46, 105, 125, 201, 346, 49, 102, 125, 346, 51, 107, 127, 202, 346, 46, 104, 125, 204, 346, 41, 101, 125, 346, 53, 104, 125, 205, 346, 20, 106, 125, 206, 346, 27, 104, 125, 346, 39, 106, 129, 208, 346, 36, 104, 125, 346, 44, 107, 125, 209, 346, 48, 104, 125, 346, 51, 109, 127, 210, 346, 44, 108, 125, 212, 346, 41, 105, 125, 346, 53, 109, 125, 213, 346, 24, 112, 125, 214, 346, 36, 106, 125, 346, 43, 109, 129, 216, 346, 39, 106, 125, 346, 48, 110, 125, 217, 346, 51, 105, 125, 346, 55, 111, 127, 218, 346, 48, 110, 125, 220, 346, 44, 108, 125, 346, 56, 112, 125, 4, 483, 189, 346, 25, 113, 125, 190, 346, 36, 109, 125, 346, 48, 112, 129, 192, 346, 41, 107, 125, 346, 53, 111, 125, 193, 346, 56, 107, 125, 346, 60, 112, 127, 194, 346, 53, 111, 125, 196, 346, 46, 112, 125, 346, 58, 112, 125, 197, 346, 26, 115, 125, 198, 346, 34, 111, 125, 346, 48, 114, 129, 200, 346, 44, 108, 125, 346, 53, 113, 125, 201, 346, 56, 108, 125, 346, 60, 114, 127, 202, 346, 53, 113, 125, 204, 346, 46, 113, 125, 346, 58, 113, 125, 205, 346, 27, 117, 125, 206, 346, 37, 112, 125, 346, 48, 114, 129, 208, 346, 43, 109, 125, 346, 51, 113, 125, 209, 346, 55, 106, 125, 346, 60, 114, 127, 210, 346, 51, 111, 125, 212, 346, 46, 109, 125, 346, 58, 113, 125, 213, 346, 15, 107, 125, 214, 346, 27, 103, 125, 346, 43, 105, 129, 216, 346, 37, 102, 125, 346, 46, 106, 125, 217, 346, 49, 103, 125, 346, 55, 107, 127, 218, 346, 46, 105, 125, 220, 346, 41, 102, 125, 346, 53, 104, 125, 4, 483, 189, 346, 20, 105, 125, 190, 346, 27, 102, 125, 346, 39, 103, 129, 192, 346, 36, 102, 125, 346, 44, 106, 125, 193, 346, 48, 101, 125, 346, 51, 107, 127, 194, 346, 44, 105, 125, 196, 346, 41, 102, 125, 346, 53, 104, 125, 197, 346, 19, 106, 125, 198, 346, 27, 103, 125, 346, 39, 105, 129, 200, 346, 37, 103, 125, 346, 46, 107, 125, 201, 346, 49, 104, 125, 346, 51, 108, 127, 202, 346, 46, 107, 125, 204, 346, 41, 103, 125, 346, 53, 105, 125, 205, 346, 20, 108, 125, 206, 346, 27, 105, 125, 346, 39, 107, 129, 208, 346, 36, 108, 125, 346, 44, 108, 125, 209, 346, 48, 105, 125, 346, 51, 110, 127, 210, 346, 44, 109, 125, 212, 346, 41, 105, 125, 346, 53, 108, 125, 213, 346, 24, 112, 125, 214, 346, 36, 109, 125, 346, 43, 109, 129, 216, 346, 39, 106, 125, 346, 48, 110, 125, 217, 346, 51, 105, 125, 346, 55, 111, 127, 218, 346, 48, 110, 125, 220, 346, 44, 106, 125, 346, 56, 110, 125, 4, 483, 189, 346, 25, 113, 125, 190, 346, 32, 110, 125, 346, 45, 110, 129, 192, 346, 41, 107, 125, 346, 49, 111, 125, 193, 346, 53, 106, 125, 346, 57, 112, 127, 194, 346, 49, 111, 125, 196, 346, 46, 107, 125, 346, 58, 111, 125, 197, 346, 27, 115, 125, 198, 346, 32, 110, 125, 346, 47, 110, 129, 200, 346, 42, 108, 125, 346, 51, 112, 125, 201, 346, 56, 107, 125, 346, 59, 113, 127, 202, 346, 51, 112, 125, 204, 346, 48, 108, 125, 346, 60, 112, 125, 205, 346, 29, 115, 125, 206, 346, 37, 111, 125, 346, 51, 111, 129, 208, 346, 44, 107, 125, 346, 53, 112, 125, 209, 346, 56, 107, 125, 346, 63, 114, 127, 210, 346, 53, 111, 125, 212, 346, 49, 108, 125, 346, 61, 113, 125, 213, 346, 27, 115, 125, 214, 346, 32, 112, 125, 346, 47, 115, 129, 216, 346, 44, 108, 125, 346, 51, 113, 125, 217, 346, 56, 108, 125, 346, 59, 115, 127, 218, 346, 51, 112, 125, 220, 346, 48, 109, 125, 346, 60, 114, 125, 4, 483, 189, 346, 29, 115, 125, 190, 346, 37, 112, 125, 346, 48, 115, 129, 192, 346, 44, 108, 125, 346, 53, 114, 125, 193, 346, 56, 108, 125, 346, 60, 116, 127, 194, 346, 53, 113, 125, 196, 346, 49, 114, 125, 346, 61, 114, 125, 197, 346, 31, 109, 125, 346, 50, 117, 131, 198, 346, 39, 110, 125, 199, 346, 55, 113, 126, 200, 346, 46, 111, 125, 201, 346, 58, 110, 125, 346, 62, 115, 127, 202, 346, 55, 113, 125, 204, 346, 51, 108, 125, 346, 63, 113, 125, 205, 346, 32, 111, 125, 346, 52, 117, 131, 206, 346, 39, 112, 125, 207, 346, 56, 113, 126, 208, 346, 48, 112, 125, 209, 346, 60, 110, 125, 346, 64, 115, 127, 210, 346, 56, 113, 125, 212, 346, 53, 108, 125, 346, 65, 114, 125, 213, 346, 34, 112, 125, 346, 54, 117, 131, 214, 346, 39, 113, 125, 215, 346, 58, 113, 126, 216, 346, 49, 113, 125, 217, 346, 63, 108, 125, 346, 66, 117, 127, 218, 346, 58, 117, 125, 220, 346, 55, 109, 125, 346, 67, 116, 125, 4, 483, 189, 346, 36, 118, 125, 190, 346, 44, 115, 125, 346, 58, 118, 129, 192, 346, 51, 110, 125, 346, 60, 117, 125, 193, 346, 63, 113, 125, 346, 70, 119, 127, 194, 346, 60, 111, 125, 196, 346, 56, 106, 125, 346, 68, 111, 125, 197, 346, 26, 111, 125, 198, 346, 35, 108, 125, 346, 55, 109, 129, 200, 346, 44, 105, 125, 346, 56, 109, 125, 201, 346, 59, 105, 125, 346, 67, 110, 127, 202, 346, 56, 108, 125, 204, 346, 53, 105, 125, 346, 65, 108, 125, 205, 346, 27, 107, 125, 346, 53, 108, 131, 206, 346, 36, 108, 125, 207, 346, 56, 107, 126, 208, 346, 44, 106, 125, 209, 346, 60, 103, 125, 346, 65, 108, 127, 210, 346, 56, 108, 125, 212, 346, 51, 104, 125, 346, 63, 107, 125, 213, 346, 27, 107, 125, 214, 346, 37, 106, 125, 346, 48, 106, 129, 216, 346, 43, 103, 125, 346, 51, 106, 125, 217, 346, 55, 101, 125, 346, 60, 107, 127, 218, 346, 51, 105, 125, 220, 346, 46, 105, 125, 346, 58, 105, 125, 4, 483, 189, 346, 20, 104, 125, 346, 39, 103, 131, 190, 346, 27, 103, 125, 191, 346, 44, 105, 126, 192, 346, 36, 101, 125, 193, 346, 48, 101, 125, 346, 51, 106, 127, 194, 346, 44, 104, 125, 196, 346, 41, 103, 125, 346, 53, 104, 125, 197, 346, 20, 104, 125, 346, 48, 103, 131, 198, 346, 27, 103, 125, 199, 346, 51, 106, 126, 200, 346, 37, 104, 125, 201, 346, 55, 102, 125, 346, 60, 108, 127, 202, 346, 51, 104, 125, 204, 346, 46, 103, 125, 346, 58, 104, 125, 205, 346, 20, 104, 125, 206, 346, 27, 103, 125, 346, 39, 103, 129, 208, 346, 36, 101, 125, 346, 44, 105, 125, 209, 346, 48, 101, 125, 346, 51, 106, 127, 210, 346, 44, 104, 125, 212, 346, 41, 103, 125, 346, 53, 104, 125, 213, 346, 20, 104, 125, 214, 346, 27, 103, 125, 346, 48, 103, 129, 216, 346, 37, 101, 125, 346, 51, 106, 125, 217, 346, 55, 102, 125, 346, 60, 108, 127, 218, 346, 51, 104, 125, 220, 346, 46, 103, 125, 346, 58, 104, 125, 4, 483, 189, 346, 20, 104, 125, 190, 346, 27, 103, 125, 346, 44, 103, 127, 192, 346, 39, 101, 125, 346, 49, 105, 125, 193, 346, 53, 101, 125, 346, 56, 106, 132, 194, 346, 49, 104, 125, 196, 346, 48, 103, 125, 346, 51, 103, 125, 197, 346, 20, 104, 125, 198, 346, 27, 103, 125, 346, 44, 103, 127, 200, 346, 39, 102, 125, 346, 49, 106, 125, 201, 346, 53, 102, 125, 346, 56, 108, 132, 202, 346, 49, 104, 125, 204, 346, 48, 103, 125, 346, 51, 103, 125, 205, 346, 20, 104, 125, 206, 346, 27, 102, 125, 346, 44, 102, 127, 208, 346, 39, 101, 125, 346, 49, 105, 125, 209, 346, 53, 101, 125, 346, 56, 105, 132, 210, 346, 49, 103, 125, 212, 346, 48, 102, 125, 346, 51, 102, 125, 213, 346, 20, 103, 125, 214, 346, 27, 102, 125, 346, 44, 102, 127, 216, 346, 39, 100, 125, 346, 49, 103, 125, 217, 346, 53, 100, 125, 346, 56, 104, 128, 218, 346, 49, 101, 125, 220, 346, 48, 101, 125, 346, 51, 101, 125, 4, 483, 189, 346, 20, 102, 140, 190, 346, 27, 101, 139, 192, 346, 32, 102, 137, 193, 346, 36, 101, 136, 194, 346, 39, 102, 135, 196, 346, 44, 101, 133, 197, 346, 48, 101, 132]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the midi files\n",
    "tokenized_midi_files = []\n",
    "for file in midi_files:\n",
    "    tok_seq: TokSequence = tokenizer.encode(file) # type: ignore    \n",
    "    tokenized_midi_files.append(tok_seq.ids) # using ids to get the integer representation of the tokens can covert back with decode\n",
    "\n",
    "# Convert the list to a NumPy array if needed\n",
    "tokenized_midi_files = np.array(tokenized_midi_files, dtype=object)\n",
    "print(tokenized_midi_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 512\n",
    "stride = max_seq_len // 2\n",
    "\n",
    "def create_chunks(tokens: list[int], max_seq_len: int, stride: int) -> list[list[int]]:\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - max_seq_len, stride):\n",
    "        chunk = tokens[i:i + max_seq_len]\n",
    "        if len(chunk) != max_seq_len:\n",
    "            print(\"error\")\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# get a list of all the chunks regardless of the midi file\n",
    "tokenized_chunks = []\n",
    "for tok_seq in tokenized_midi_files:\n",
    "    chunks = create_chunks(tok_seq, max_seq_len, stride)\n",
    "    \n",
    "    # add the chunks to the list of chunks\n",
    "    tokenized_chunks.extend(chunks)\n",
    "    \n",
    "# convert the list to a NumPy array\n",
    "tokenized_chunks = np.array(tokenized_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianmh\\AppData\\Local\\Temp\\ipykernel_20116\\3835522063.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  input_tensor = torch.tensor(input_data, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# get the input and target data from the chunks using a autoregressive model\n",
    "input_data = [chunk[:-1] for chunk in tokenized_chunks]\n",
    "target_data = [chunk[1:] for chunk in tokenized_chunks]\n",
    "\n",
    "# convert the input and target data to tensors\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.long)\n",
    "target_tensor = torch.tensor(target_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Spilt the data into training and validation and testing sets for inputs and targets\n",
    "train_input, vt_input, train_target, vt_target = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=seed)\n",
    "val_input, test_input, val_target, test_target = train_test_split(vt_input, vt_target, test_size=0.5, random_state=seed)\n",
    "\n",
    "# Create the dataset class\n",
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.inputs[idx], \"labels\": self.targets[idx]}\n",
    "    \n",
    "# Create the datasets\n",
    "train_dataset = MidiDataset(train_input, train_target)\n",
    "val_dataset = MidiDataset(val_input, val_target)\n",
    "test_dataset = MidiDataset(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Define model configuration\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,  # Total number of unique tokens\n",
    "    n_positions=max_seq_len,  # Max sequence length\n",
    "    n_embd=768,  # Embedding size\n",
    "    n_layer=12,  # Number of transformer layers\n",
    "    n_head=12   # Number of attention heads\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Output directory to save model and logs\n",
    "    num_train_epochs=3,  # Number of epochs\n",
    "    per_device_train_batch_size=8,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # Batch size per device during evaluation\n",
    "    warmup_steps=500,    # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,   # Strength of weight decay\n",
    "    logging_dir='./logs', # Directory for storing logs\n",
    "    logging_steps=10,    # Log every 10 steps\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model when finished training\n",
    "    metric_for_best_model=\"loss\",  # Metric to track for best model\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # Your GPT2 model\n",
    "    args=training_args,  # The training arguments\n",
    "    train_dataset=train_dataset,  # The training set (TensorDataset)\n",
    "    eval_dataset=val_dataset,  # The validation set (TensorDataset)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the final model path\n",
    "final_model_path = f\"./{artist}_final_model\"\n",
    "\n",
    "# check if the final model is saved\n",
    "if not os.path.exists(final_model_path):\n",
    "    trainer.train()\n",
    "    trainer.save_model(final_model_path)  # Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a4ef7b954345139348043d2d338f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.226705551147461,\n",
       " 'eval_model_preparation_time': 0.0,\n",
       " 'eval_runtime': 5.1968,\n",
       " 'eval_samples_per_second': 28.094,\n",
       " 'eval_steps_per_second': 3.656}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer\n",
    "\n",
    "# load the final model from the saved model\n",
    "model = GPT2LMHeadModel.from_pretrained(final_model_path)\n",
    "\n",
    "# evaluate the model on the test dataset using the loaded model\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to be evaluated\n",
    "    args=training_args,  # The evaluation arguments\n",
    "    eval_dataset=test_dataset,  # The test dataset\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the final model from the saved model\n",
    "model = GPT2LMHeadModel.from_pretrained(final_model_path)\n",
    "\n",
    "# get the first 512 tokens from the test dataset\n",
    "seed_tokens = test_dataset[100][\"input_ids\"].unsqueeze(0)\n",
    "\n",
    "# create the input_ids tensor\n",
    "input_ids = seed_tokens\n",
    "\n",
    "# function to generate 1 new id at end of the input_ids tensor\n",
    "def generated_ids(ids):\n",
    "    # Generate tokens\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=ids,  # Your seed sequence\n",
    "        max_length=max_seq_len,  # Maximum length of generated song (in tokens)\n",
    "        num_return_sequences=1,  # Number of sequences to generate\n",
    "        no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
    "        do_sample=True,  # Enable sampling\n",
    "        temperature=1.0,  # Sampling temperature for randomness (higher = more randomness)\n",
    "        top_p=0.95,  # Use top-p sampling for diversity\n",
    "        top_k=50,  # Use top-k sampling for diversity\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Padding token id if needed\n",
    "    )\n",
    "\n",
    "    return generated_ids[0].tolist()  # Get the generated tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|██████████| 100/100 [00:30<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# for loop to generate new tokens\n",
    "final_tokens = input_ids[0].tolist() # initial length of 511\n",
    "new_ids = input_ids\n",
    "for i in tqdm(range(100), desc=\"Generating tokens\"):\n",
    "    # get the new tokens\n",
    "    new_tokens = generated_ids(new_ids)\n",
    "    \n",
    "    # check the length of the new tokens\n",
    "    if len(new_tokens) != 512:\n",
    "        print(f\"error: {len(new_tokens)}\")\n",
    "        break\n",
    "    \n",
    "    # check the previous tokens equal the new tokens\n",
    "    if final_tokens[i+1:] != new_tokens[1:-1]:\n",
    "        print(final_tokens[i+1:])\n",
    "        print(new_tokens[1:-1])\n",
    "        print(\"error: tokens are not the same\")\n",
    "        break\n",
    "    \n",
    "    # get the new token\n",
    "    new_token = new_tokens[-1]\n",
    "    \n",
    "    # add the new token to the final tokens\n",
    "    final_tokens.append(new_token)\n",
    "    \n",
    "    # shift the window by one token\n",
    "    new_ids = torch.tensor(final_tokens[(i+1):]).unsqueeze(0)\n",
    "    \n",
    "    # make sure the input_ids length is length 511\n",
    "    if len(new_ids[0]) != 511:\n",
    "        print(f\"error: {len(new_ids)}\")\n",
    "        break\n",
    "        \n",
    "# print the final tokens length\n",
    "print(len(final_tokens))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105, 126, 209, 346, 52, 106, 126, 211, 346, 43, 105, 126, 213, 346, 48, 106, 126, 215, 346, 43, 105, 126, 217, 346, 40, 106, 126, 219, 346, 31, 105, 126, 4, 483, 189, 346, 17, 107, 156, 346, 29, 109, 156, 191, 346, 36, 105, 126, 193, 346, 41, 106, 126, 195, 346, 45, 105, 126, 197, 346, 51, 108, 126, 199, 346, 48, 105, 126, 201, 346, 53, 106, 126, 203, 346, 57, 105, 126, 205, 346, 63, 108, 126, 207, 346, 60, 105, 126, 209, 346, 65, 106, 126, 211, 346, 69, 105, 126, 213, 346, 75, 108, 126, 215, 346, 69, 105, 126, 217, 346, 65, 107, 126, 219, 346, 60, 105, 126, 4, 483, 189, 346, 75, 109, 126, 346, 22, 107, 156, 346, 34, 110, 156, 191, 346, 68, 105, 126, 193, 346, 65, 107, 126, 195, 346, 60, 105, 126, 197, 346, 63, 109, 126, 199, 346, 56, 106, 126, 201, 346, 53, 107, 126, 203, 346, 48, 106, 126, 205, 346, 51, 109, 126, 207, 346, 44, 106, 126, 209, 346, 41, 107, 126, 211, 346, 36, 106, 126, 213, 346, 50, 108, 126, 215, 346, 44, 106, 126, 217, 346, 41, 106, 126, 219, 346, 34, 105, 126, 4, 483, 189, 346, 15, 109, 172, 346, 27, 112, 172, 191, 346, 34, 107, 126, 193, 346, 39, 108, 126, 195, 346, 44, 107, 126, 197, 346, 49, 110, 126, 199, 346, 46, 107, 126, 201, 346, 51, 108, 126, 203, 346, 56, 108, 126, 205, 346, 61, 111, 126, 207, 346, 58, 108, 126, 209, 346, 63, 109, 126, 211, 346, 68, 108, 126, 213, 346, 73, 110, 126, 215, 346, 70, 108, 126, 217, 346, 75, 109, 126, 219, 346, 80, 108, 126, 4, 483, 189, 346, 85, 111, 126, 191, 346, 79, 108, 126, 193, 346, 75, 109, 126, 195, 346, 70, 108, 126, 197, 346, 73, 111, 126, 199, 346, 67, 109, 126, 201, 346, 63, 109, 126, 203, 346, 58, 108, 126, 205, 346, 61, 110, 126, 207, 346, 55, 108, 126, 209, 346, 51, 109, 126, 211, 346, 46, 109, 126, 213, 346, 49, 110, 126, 215, 346, 43, 108, 126, 217, 346, 39, 109, 126, 219, 346, 34, 108, 126, 4, 483, 189, 346, 32, 116, 128, 346, 20, 112, 156, 191, 346, 32, 110, 154, 193, 346, 39, 111, 126, 195, 346, 42, 110, 126, 197, 346, 48, 112, 126, 199, 346, 44, 110, 126, 201, 346, 51, 111, 126, 203, 346, 54, 111, 126, 205, 346, 60, 114, 126, 207, 346, 56, 111, 126, 209, 346, 63, 112, 126, 211, 346, 66, 111, 126, 213, 346, 72, 114, 126, 215, 346, 68, 111, 126, 217, 346, 75, 112, 126, 219, 346, 80, 111, 126, 4, 483, 189, 346, 83, 115, 126, 346, 14, 113, 156, 346, 26, 116, 156, 191, 346, 78, 111, 126, 193, 346, 71, 112, 126, 195, 346, 68, 111, 126, 197, 346, 71, 114, 126, 199, 346, 66, 111, 126, 201, 346, 59, 112, 126, 203, 346, 56, 111, 205, 29, 126, 49, 126, 59, 130, 346, 111, 211, 60, 126, 57, 126, 475, 346, 109, 346, 106, 205, 56, 126, 44, 126, 46, 128, 43, 126, 32, 126, 56, 128, 42, 126, 39, 128, 49, 128, 60, 128, 44, 130, 63, 126, 70, 126, 48, 126, 61, 130, 57, 128, 50, 126, 53, 126, 37, 126, 483, 346, 112, 205, 46, 126, 50, 130, 480, 346, 110, 209, 55, 126, 51, 128, 65, 126, 63, 130, 49, 130, 46, 140, 346, 105, 346, 114, 346, 108, 205, 30, 128, 34, 132, 59, 126, 58, 126, 62, 126, 72]\n",
      "[105, 126, 209, 346, 52, 106, 126, 211, 346, 43, 105, 126, 213, 346, 48, 106, 126, 215, 346, 43, 105, 126, 217, 346, 40, 106, 126, 219, 346, 31, 105, 126, 4, 483, 189, 346, 17, 107, 156, 346, 29, 109, 156, 191, 346, 36, 105, 126, 193, 346, 41, 106, 126, 195, 346, 45, 105, 126, 197, 346, 51, 108, 126, 199, 346, 48, 105, 126, 201, 346, 53, 106, 126, 203, 346, 57, 105, 126, 205, 346, 63, 108, 126, 207, 346, 60, 105, 126, 209, 346, 65, 106, 126, 211, 346, 69, 105, 126, 213, 346, 75, 108, 126, 215, 346, 69, 105, 126, 217, 346, 65, 107, 126, 219, 346, 60, 105, 126, 4, 483, 189, 346, 75, 109, 126, 346, 22, 107, 156, 346, 34, 110, 156, 191, 346, 68, 105, 126, 193, 346, 65, 107, 126, 195, 346, 60, 105, 126, 197, 346, 63, 109, 126, 199, 346, 56, 106, 126, 201, 346, 53, 107, 126, 203, 346, 48, 106, 126, 205, 346, 51, 109, 126, 207, 346, 44, 106, 126, 209, 346, 41, 107, 126, 211, 346, 36, 106, 126, 213, 346, 50, 108, 126, 215, 346, 44, 106, 126, 217, 346, 41, 106, 126, 219, 346, 34, 105, 126, 4, 483, 189, 346, 15, 109, 172, 346, 27, 112, 172, 191, 346, 34, 107, 126, 193, 346, 39, 108, 126, 195, 346, 44, 107, 126, 197, 346, 49, 110, 126, 199, 346, 46, 107, 126, 201, 346, 51, 108, 126, 203, 346, 56, 108, 126, 205, 346, 61, 111, 126, 207, 346, 58, 108, 126, 209, 346, 63, 109, 126, 211, 346, 68, 108, 126, 213, 346, 73, 110, 126, 215, 346, 70, 108, 126, 217, 346, 75, 109, 126, 219, 346, 80, 108, 126, 4, 483, 189, 346, 85, 111, 126, 191, 346, 79, 108, 126, 193, 346, 75, 109, 126, 195, 346, 70, 108, 126, 197, 346, 73, 111, 126, 199, 346, 67, 109, 126, 201, 346, 63, 109, 126, 203, 346, 58, 108, 126, 205, 346, 61, 110, 126, 207, 346, 55, 108, 126, 209, 346, 51, 109, 126, 211, 346, 46, 109, 126, 213, 346, 49, 110, 126, 215, 346, 43, 108, 126, 217, 346, 39, 109, 126, 219, 346, 34, 108, 126, 4, 483, 189, 346, 32, 116, 128, 346, 20, 112, 156, 191, 346, 32, 110, 154, 193, 346, 39, 111, 126, 195, 346, 42, 110, 126, 197, 346, 48, 112, 126, 199, 346, 44, 110, 126, 201, 346, 51, 111, 126, 203, 346, 54, 111, 126, 205, 346, 60, 114, 126, 207, 346, 56, 111, 126, 209, 346, 63, 112, 126, 211, 346, 66, 111, 126, 213, 346, 72, 114, 126, 215, 346, 68, 111, 126, 217, 346, 75, 112, 126, 219, 346, 80, 111, 126, 4, 483, 189, 346, 83, 115, 126, 346, 14, 113, 156, 346, 26, 116, 156, 191, 346, 78, 111, 126, 193, 346, 71, 112, 126, 195, 346, 68, 111, 126, 197, 346, 71, 114, 126, 199, 346, 66, 111, 126, 201, 346, 59, 112, 126, 203, 346, 56, 111]\n",
      "[]\n",
      "Score(ttype=Tick, tpq=16, begin=0, end=448, tracks=1, notes=100, time_sig=4, key_sig=0, markers=0)\n",
      "Score(ttype=Tick, tpq=16, begin=0, end=448, tracks=1, notes=100, time_sig=1, key_sig=0, markers=0)\n"
     ]
    }
   ],
   "source": [
    "from symusic.core import ScoreTick\n",
    "print(final_tokens)\n",
    "print(input_ids[0].tolist())\n",
    "\n",
    "decoded_tokens = []\n",
    "for token in final_tokens:\n",
    "    decoded_token = tokenizer.token_id_type(id_=token)\n",
    "print(decoded_tokens)\n",
    "\n",
    "# get the tokens from the generated tokens\n",
    "midi_sequence: ScoreTick = tokenizer.decode(final_tokens) \n",
    "print(midi_sequence)\n",
    "\n",
    "# get the path to the output file\n",
    "midi_file = \"multi_generated_midi.mid\"\n",
    "midi_path = os.path.join(cwd, midi_file)\n",
    "\n",
    "# save the midi sequence to a midi file\n",
    "midi_sequence.dump_midi(midi_path)\n",
    "\n",
    "# re\n",
    "\n",
    "# also decode the input_ids to get the original midi sequence\n",
    "input_midi_sequence: ScoreTick = tokenizer.decode(seed_tokens[0].tolist())\n",
    "print(input_midi_sequence)\n",
    "\n",
    "# get the path to the input file\n",
    "input_midi_file = \"input_midi.mid\"\n",
    "input_midi_path = os.path.join(cwd, input_midi_file)\n",
    "\n",
    "# save the input midi sequence to a midi file\n",
    "input_midi_sequence.dump_midi(input_midi_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
